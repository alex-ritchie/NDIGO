{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture modeling with GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.mixture\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn\n",
    "\n",
    "#import code to generate data and do clustNP\n",
    "from clustNP import clustNP, gauss_kernal_mat, gen_ZG, gen_C, gen_Ci, clustNP_obj, proj_simplex\n",
    "\n",
    "from polyglot.mapping import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 10d Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embedding.from_glove(\"glove10d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Twitter Russian Trolls (run commented cells to recreate data matrix if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# path = r'russian-troll-tweets-master' # use your path\n",
    "# all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# li = []\n",
    "\n",
    "# for filename in all_files:\n",
    "#     df = pd.read_csv(filename, index_col=None, header=0)\n",
    "#     li.append(df)\n",
    "\n",
    "# frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "# frame.to_pickle('tweets_frame.pkl')\n",
    "\n",
    "# to load\n",
    "# frame = pd.read_pickle('tweets_frame.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # tokenize, lemmatize\n",
    "# import nltk\n",
    "\n",
    "# w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_text(text):\n",
    "#     return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "# df_en = frame.loc[frame['language'] == 'English']\n",
    "# df_16 = df_en[df_en['publish_date'].str.contains('2016')]\n",
    "# df_16['content']=df_16.content.apply(lemmatize_text)\n",
    "# df_16_content = df_16['content']\n",
    "# df_16.shape #878878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_16_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pair words\n",
    "# import random\n",
    "# import string\n",
    "# np.random.seed(0)\n",
    "# punct = string.punctuation\n",
    "\n",
    "# max_pairs = 5000000\n",
    "# X = np.zeros((max_pairs,10)) #take 5mil word pairs max\n",
    "# vs = np.zeros((280,10)) #preallocate max tweet char length array\n",
    "# X_words = [] #store the words for each entry of X\n",
    "# idx=0\n",
    "# for j,twt in enumerate(df_16_content):\n",
    "#     if np.mod(j,1000) == 0:\n",
    "#         print(j)\n",
    "#     l = len(twt)\n",
    "#     random.shuffle(twt)\n",
    "#     for i,word in enumerate(twt):\n",
    "#         try:\n",
    "#             word = word.strip(punct)\n",
    "#             word = word.strip('#')\n",
    "#             v = embeddings[word]\n",
    "#             vs[i]  = v\n",
    "#             X_words.append(word)\n",
    "#             if np.sum(np.abs(v)) == 0:\n",
    "#                 print(v)\n",
    "#         except:\n",
    "#             l-=1\n",
    "#     l-=np.mod(l,2)\n",
    "#     #stacking an even number of vectors from each tweet, can assign pairs ids after the fact\n",
    "#     if l>0 and idx+l <= max_pairs:\n",
    "#         X[idx:idx+l,:] = vs[:l]\n",
    "#         idx+=l\n",
    "#     elif l <= 0:\n",
    "#         pass\n",
    "#     else:\n",
    "#         print(\"done\")\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##len(X_words) #3829919\n",
    "##X_words = X_words[:-1] \n",
    "#print word pairs to a txt file to use with other methods\n",
    "# lines = []\n",
    "# for i in range(0,len(X_words)-1,2):\n",
    "#     words = '%s %s\\n'%(X_words[i], X_words[i+1])\n",
    "#     lines.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"troll-corpus.txt\", \"w\")\n",
    "# f.writelines(lines)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# to run java code:\n",
    "\n",
    "# GPU-DMM: $ java -jar jar/STTM.jar -model GPUDMM -corpus dataset/troll-corpus.txt -vectors ../glove10d.txt -ntopics 10 -niters 1000 -name 'gpudmm'\n",
    "\n",
    "# LF-DMM: $ java -jar jar/STTM.jar -model LFDMM -corpus dataset/troll-corpus.txt -vectors ../glove10d.txt -ntopics 10 -niters 1000 -name 'lfdmm'\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # remove extra rows\n",
    "# # pair_ids = np.array([i for i in range(X.shape[0]//2) for j in range(2)])\n",
    "# num_unfilled = np.sum(np.sum(np.abs(X),axis=1)==0) #1577844\n",
    "# # X = X[:(max_pairs-num_unfilled),:]\n",
    "# print(num_unfilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove any cells that were filled with zeros\n",
    "#np.where(np.sum(np.abs(X),axis=1)==0) # (array([ 3,  4,  5,  9, 10, 20, 26, 32]),)\n",
    "# X = np.delete(X,[2,3,4,5,8,9,10,11,20,21,26,27,32,33],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # save\n",
    "# # print(X.shape) #(3422142, 10)\n",
    "# with open('tweets_preprocessed_lemmatized.npz', 'wb') as f:\n",
    "# #     np.save(f, X)\n",
    "#     np.savez_compressed(f, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Param Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('tweets_preprocessed_lemmatized.npz') as data:\n",
    "    X = data[\"arr_0\"]\n",
    "pair_ids = np.array([i for i in range(X.shape[0]//2) for j in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# params\n",
    "R = 200\n",
    "best_ISE = 1e10;\n",
    "d = X.shape[1]\n",
    "M = 10\n",
    "sd = np.mean(np.std(X, axis=0))\n",
    "sigma = sd*(R)**(-1/(d+4)) #scott's rule of thumb\n",
    "pair_ids = np.array([i for i in range(X.shape[0]//2) for j in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4484476088905496 200 0.018922095717083417\n"
     ]
    }
   ],
   "source": [
    "#use random batch of 10000 pairs to initialize G,Z\n",
    "\n",
    "# five seeds for five experiments\n",
    "np.random.seed(0); results_filename = 'NDIGO_seed0';\n",
    "#np.random.seed(1)\n",
    "#np.random.seed(2)\n",
    "# np.random.seed(3)\n",
    "\n",
    "\n",
    "n_init = 10000\n",
    "#n_init = X.shape[0]//2\n",
    "\n",
    "rand_pairs = np.random.permutation(pair_ids.shape[0]//2)[:n_init]\n",
    "idxs = 2*(np.maximum(rand_pairs-1, 0))\n",
    "temp = np.append(np.insert(idxs, slice(1, None), 0), 0)\n",
    "temp[1::2] = idxs+1\n",
    "idxs = temp\n",
    "Xmini = X[idxs,:]\n",
    "rand_pair_ids = [pid for j in range(2) for pid in rand_pairs]\n",
    "Z, G = gen_ZG(Xmini, R, sigma)\n",
    "\n",
    "#Generate initialization\n",
    "C = gen_C(Xmini, rand_pair_ids, Z, R, sigma)\n",
    "# C = np.sum(Ci, axis=2)\n",
    "# Cinv = np.linalg.inv(C)\n",
    "# A, w, _ = scipy.sparse.linalg.svds(G@Cinv@G, k=M)\n",
    "\n",
    "w, A = scipy.sparse.linalg.eigs(G, k=M)\n",
    "w = np.real(w)\n",
    "A = np.real(A)\n",
    "for i in range(M):\n",
    "    A[:,i] = proj_simplex(A[:,i])\n",
    "#w = w/np.sum(w)\n",
    "w = np.ones(M,) / M\n",
    "A = A / np.sum(A,axis=0)\n",
    "f, _ = clustNP_obj(A, w, G, C, n_init, 0)\n",
    "\n",
    "best_ISE = f\n",
    "best_sigma = sigma\n",
    "best_R = R\n",
    "best_w = w\n",
    "best_Z = Z\n",
    "best_C = C\n",
    "best_G = G\n",
    "best_A = A\n",
    "print(best_sigma, best_R, best_ISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tweets_best_R_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, best_R)\n",
    "# with open('tweets_best_w_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, best_w)\n",
    "# with open('tweets_best_Z_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, best_Z)\n",
    "# with open('tweets_best_G_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, best_G)\n",
    "# with open('tweets_best_A_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, best_A)\n",
    "# with open('tweets_best_sigma_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, best_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018922095717083417\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "0.00040102995878448866\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "0.00018490087865764848\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "0.00013107076686919068\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "0.00010901645263253318\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "9.790277923335766e-05\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "9.151861054979631e-05\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "8.746510053687119e-05\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "8.465714330941375e-05\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "8.268780379034414e-05\n",
      "processed observations this epoch:  512000\n",
      "processed observations this epoch:  1024000\n",
      "processed observations this epoch:  1536000\n",
      "8.121397403392084e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.121397403392084e-05"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = best_sigma\n",
    "w0 = best_w\n",
    "R = best_R\n",
    "#w0 = np.array([0.5, 0.5])\n",
    "Z = best_Z\n",
    "G = best_G\n",
    "A0 = best_A\n",
    "#A0 = np.random.rand(R,M)\n",
    "#A0 = A0 / np.sum(A0, axis=0)\n",
    "ss=0.03\n",
    "\n",
    "print(best_ISE)\n",
    "# #solve\n",
    "f_star, A_star, w_star, Z, n_iter = clustNP(X, pair_ids, A0, w0, Z, G, stepsize=ss, ss_decr=1, \n",
    "                                            epoch_decr = 40, method='psgd', max_iter=10, f_tol=1e-16, \n",
    "                                            grad_tol=1e-8, R=R, sigma=sigma, batch_size=1024, backtrack=False,\n",
    "                                            decay=0.000001, momentum=0.2, large=True)\n",
    "# f_star, A_star, w_star, Z, n_iter = clustNP(X, pair_ids, A_star, w_star, Z, G, stepsize=ss, ss_decr=1, \n",
    "#                                             epoch_decr = 40, method='psgd', max_iter=30, f_tol=1e-16, \n",
    "#                                             grad_tol=1e-8, R=R, sigma=sigma, batch_size=1024, backtrack=False,\n",
    "#                                             decay=0.00001, momentum=0.2, large=True)\n",
    "f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tweets_A_star_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, A_star)\n",
    "# with open('tweets_w_star_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, w_star)\n",
    "# with open('tweets_f_star_other_4.npy', 'wb') as f:\n",
    "#     np.save(f, f_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_star = np.load('tweets_A_star_other.npy')\n",
    "# w_star = np.load('tweets_w_star_other.npy')\n",
    "# A_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "2280000\n",
      "2290000\n",
      "2300000\n",
      "2310000\n",
      "2320000\n",
      "2330000\n",
      "2340000\n",
      "2350000\n",
      "2360000\n",
      "2370000\n",
      "2380000\n",
      "2390000\n",
      "2400000\n",
      "2410000\n",
      "2420000\n",
      "2430000\n",
      "2440000\n",
      "2450000\n",
      "2460000\n",
      "2470000\n",
      "2480000\n",
      "2490000\n",
      "2500000\n",
      "2510000\n",
      "2520000\n",
      "2530000\n",
      "2540000\n",
      "2550000\n",
      "2560000\n",
      "2570000\n",
      "2580000\n",
      "2590000\n",
      "2600000\n",
      "2610000\n",
      "2620000\n",
      "2630000\n",
      "2640000\n",
      "2650000\n",
      "2660000\n",
      "2670000\n",
      "2680000\n",
      "2690000\n",
      "2700000\n",
      "2710000\n",
      "2720000\n",
      "2730000\n",
      "2740000\n",
      "2750000\n",
      "2760000\n",
      "2770000\n",
      "2780000\n",
      "2790000\n",
      "2800000\n",
      "2810000\n",
      "2820000\n",
      "2830000\n",
      "2840000\n",
      "2850000\n",
      "2860000\n",
      "2870000\n",
      "2880000\n",
      "2890000\n",
      "2900000\n",
      "2910000\n",
      "2920000\n",
      "2930000\n",
      "2940000\n",
      "2950000\n",
      "2960000\n",
      "2970000\n",
      "2980000\n",
      "2990000\n",
      "3000000\n",
      "3010000\n",
      "3020000\n",
      "3030000\n",
      "3040000\n",
      "3050000\n",
      "3060000\n",
      "3070000\n",
      "3080000\n",
      "3090000\n",
      "3100000\n",
      "3110000\n",
      "3120000\n",
      "3130000\n",
      "3140000\n",
      "3150000\n",
      "3160000\n",
      "3170000\n",
      "3180000\n",
      "3190000\n",
      "3200000\n",
      "3210000\n",
      "3220000\n",
      "3230000\n",
      "3240000\n",
      "3250000\n",
      "3260000\n",
      "3270000\n",
      "3280000\n",
      "3290000\n",
      "3300000\n",
      "3310000\n",
      "3320000\n",
      "3330000\n",
      "3340000\n",
      "3350000\n",
      "3360000\n",
      "3370000\n",
      "3380000\n",
      "3390000\n",
      "3400000\n",
      "3410000\n",
      "3420000\n"
     ]
    }
   ],
   "source": [
    "#make predictions\n",
    "M=10\n",
    "phats_train = np.zeros((X.shape[0],M))\n",
    "for i in range(X.shape[0]):\n",
    "    if np.mod(i, 10000) == 0:\n",
    "        print(i)\n",
    "    kx = gauss_kernal_mat(X[i, np.newaxis], best_Z, best_sigma)\n",
    "    phats_train[i, :] = kx@A_star\n",
    "ltr = np.zeros((X.shape[0], M))\n",
    "ltr = w_star.T*phats_train\n",
    "# decisions_train = np.argmax(ltr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : ['yet', 'else', 'anyone', 'find', 'everyone', 'life', 'always', 'lot', 'exactly', 'none', 'certainly', 'sure', 'nothing', 'might', 'anything', 'fact', 'cut', 'wa', 'done', 'thought']\n",
      "topic 1 : ['might', 'even', 'make', 'still', 'yet', 'get', 'could', 'good', 'going', 'life', 'right', 'lot', 'better', 'way', 'know', 'making', 'never', 'come', 'want', 'find']\n",
      "topic 2 : ['return', 'ended', '12', 'win', '20', 'next', '15', '30', 'year', 'including', '10', 'ago', '11', '18', 'help', 'month', '16', '14', 'start', 'early']\n",
      "topic 3 : ['support', 'become', 'terms', 'must', 'every', 'huge', 'related', 'problem', 'cannot', 'growth', 'particular', 'fact', 'special', 'changes', 'strong', 'reasons', 'details', 'involved', 'possible', 'politics']\n",
      "topic 4 : ['say', 'want', 'go', 'know', 'going', 'good', 'world', 'believe', 'get', 'come', 'like', 'might', 'better', 'recent', 'hard', 'let', 'think', 'best', 'big', 'make']\n",
      "topic 5 : ['say', 'want', 'news', 'wa', 'shit', 'university', 'quaalude', 'implanted', 'p2', 'ha', 'hunting', 'heroin', 'hitter', 'islam', 'activist', 'newscaster', 'astro', 'highway', 'daily', 'nice']\n",
      "topic 6 : ['world', 'news', 'games', 'european', 'eighth', 'tried', 'suffered', 'wa', 'university', 'quaalude', 'implanted', 'shit', 'p2', 'activist', 'heroin', 'islam', 'hunting', 'hitter', 'highway', 'newscaster']\n",
      "topic 7 : ['35', '70', '16', '13', '21', '28', '19', '32', '17', '14', '23', 'win', '31', '55', '22', '65', '27', '75', '33', '29']\n",
      "topic 8 : ['train', 'deadline', 'bus', '150', 'initiative', 'contemporary', 'literary', 'papers', 'excluding', 'initiatives', 'maps', '300', 'allows', 'competitive', 'charity', 'goodwill', 'earning', 'earn', 'manage', 'practical']\n",
      "topic 9 : ['hold', 'amid', 'continue', 'bring', 'inside', 'among', 'allow', 'hundreds', 'violence', 'key', 'push', 'supposed', 'deadly', 'recent', 'laws', 'decade', 'seeking', 'trend', 'allowed', 'linked']\n"
     ]
    }
   ],
   "source": [
    "# predicting individual training words\n",
    "X_unique, idx_unique = np.unique(X, axis=0, return_index=True)\n",
    "ltr_unique = ltr[idx_unique,:]\n",
    "a = 20\n",
    "top_words_topics = []\n",
    "for m in range(M):\n",
    "    top_a_idx = ltr_unique[:,m].argsort()[-a:][::-1]\n",
    "    words = [embeddings.words[np.where((embeddings.vectors == tuple(X_unique[i,:])).all(axis=1))[0][0]] \n",
    "             for i in list(top_a_idx)]\n",
    "    top_words_topics.append(' '.join(words) + '\\n')\n",
    "    print('topic %i :' %m, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstr = \"STTM-master/NDIGOresults/\" + results_filename + \".topWords\"\n",
    "f = open(fstr, \"w\") # this can be used directly with the topic coherence score function in STTP\n",
    "f.writelines(top_words_topics)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
